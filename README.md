# Human-Action-Recognition
This project is for Computer Vision course - Innopolis University - S23

This project uses Detectron2 and LSTM to recognize human actions in videos. 
Detectron2: a powerful object detection library that is capable of detecting humans in video frames. [a link](https://github.com/facebookresearch/detectron2)
LSTM: a type of recurrent neural network that is capable of processing sequences of inputs such as time-series data.
Dataset: RNN for Human Activity Recognition - 2D Pose Input [a link](https://github.com/stuarteiffert/RNN-for-Human-Activity-Recognition-using-2D-Pose-Input#dataset-overview)

This dataset is comprised of 12 subjects doing the following 6 actions for 5 repetitions, filmed from 4 angles, repeated 5 times each.

 * JUMPING
 * JUMPING_JACKS
 * BOXING
 * BOXING
 * WAVING_2HANDS
 * WAVING_1HAND
 * CLAPPING_HANDS


 The below image is an example of the 4 camera views during the 'boxing' action for subject 1
![image](https://github.com/stuarteiffert/RNN-for-Human-Activity-Recognition-using-2D-Pose-Input/blob/master/images/boxing_all_views.gif)

















